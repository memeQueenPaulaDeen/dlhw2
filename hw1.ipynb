{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display, Math, Latex\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1>HomeWork #2</h1>\n",
    "<h2>Sami Wood</h2>\n",
    "\n",
    "<h3>Problem 1</h3>\n",
    "\n",
    "\n",
    "\n",
    "<h4>Part A:</h4>\n",
    "Build a Naïve Bayes classifierusing the data shown in Table 1to predict whether a text filewill be saved (class label y = +1) or discarded(y = -1). The features (x(i), i = 1, 2, ..., 5) are binary values (representing some attributes of a text file, e.g., “known author or not”, “long or short”, etc.).\n",
    "\n",
    "\n",
    "The classifier computes the probability in the following steps:\n",
    "\n",
    "<li>Step 1: Compute the prior probability for each class(p(y)), y+-1 or +1;</li>\n",
    "<li>Step 2: Computethe likelihood probability with each classfor each feature (p(x(i)| y))</li>\n",
    "<li>Step 3: Calculate the posterior probability for each class given each feature (p(y | x(i));</li>\n",
    "<li>Step 4: Predictaclass for a given text filebased on the posterior probability. Specifically, what is the posterior probability that y = +1 given the feature vectorx = (1 1 0 1 0). Which class would be predicted for x = (0 0 0 0 0)? What about for x = (1 1 0 1 0)</li>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'x1':[0,1,0,1,0,1,0,1,1,1],\n",
    "                   'x2':[0,1,1,1,1,0,0,0,0,1],\n",
    "                   'x3':[1,0,1,1,0,1,1,0,1,1],\n",
    "                   'x4':[1,1,1,1,0,1,0,0,1,1],\n",
    "                   'x5':[0,0,1,0,0,1,0,0,0,1],\n",
    "                   'y' :[-1,-1,-1,-1,-1,1,1,1,1,-1]})\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<b>Step 1</b>\n",
    "calculate prior for each class:\n",
    "\n",
    "In this case we have only two classes and we can simply look at the relative frequencies of each class.\n",
    "We would do this with the following equation:\n",
    "\n",
    "$$\\frac{NumInClass}{Total}$$\n",
    "\n",
    "The calculation is shown below"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prior for class 1: 0.4\n",
      "prior for class -1: 0.6\n"
     ]
    }
   ],
   "source": [
    "p1 = len(df.loc[df['y'] == 1])/len(df)\n",
    "pm1 = len(df.loc[df['y'] == -1])/len(df)\n",
    "print('prior for class 1: ' + str(p1))\n",
    "print('prior for class -1: ' + str(pm1))\n",
    "priorDf = pd.DataFrame({'y':[1,-1],'prob':[p1,pm1]})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<b>Step 2:</b>\n",
    "In order to find the probability of a class given some input we can apply bayes rule:\n",
    "\n",
    "$$P(y_i|x_1,x_2...,x_n) = P(x_1,x_2,...,x_n) * P(y_i)$$\n",
    "\n",
    "This calculation can be greatly simplified if we make the assumption that all the input variables x are independent.\n",
    "\n",
    "$$P(y_i|x_1,x_2,...,x_n) = P(x_1|y_i)*P(x_2|y_i)*...*P(x_n|y_i)*P(y_i)$$\n",
    "\n",
    "Now we can work with this equation as we can calculate the individual probability distribution terms as follows:\n",
    "\n",
    "$$P(x_j|y_i) = count(x_i)/count(y)$$\n",
    "\n",
    "when done for each variable $x_j$ with $y_i$ we will have a binary probability distribution for each variable $x_j$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      x1        x2        x3        x4        x5  y\n",
      "x1                                                 \n",
      "0   0.25  1.000000  0.250000  0.500000  0.750000  1\n",
      "1   0.75  0.000000  0.750000  0.500000  0.250000  1\n",
      "0   0.50  0.166667  0.333333  0.166667  0.666667 -1\n",
      "1   0.50  0.833333  0.666667  0.833333  0.333333 -1\n"
     ]
    }
   ],
   "source": [
    "pdf = pd.DataFrame([])\n",
    "for y in set(df.y):\n",
    "    mini = df.loc[df.y == y]\n",
    "    pdfmini = pd.DataFrame([])\n",
    "    u = len(mini)\n",
    "    for x in df.columns:\n",
    "        if x != 'y':\n",
    "            pdfmini[x]  = mini.groupby(x).size()/u\n",
    "    pdfmini['y'] = y\n",
    "    pdf = pdf.append(pdfmini)\n",
    "\n",
    "pdf.fillna(0,inplace=True)\n",
    "print(pdf)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<b>Step 3:</b>\n",
    "Now that we have a probability distribution of our inputs given the observation of our outputs we can apply bayes rule to find a probability distrobution of outputs given inputs.\n",
    "Again this is simply calculated assuming the inputs are independent as follows:\n",
    "\n",
    "$$P(y_i|x_1,x_2,...,x_n) = P(x_1|y_i)*P(x_2|y_i)*...*P(x_n|y_i)*P(y_i)$$\n",
    "\n",
    "For the given data we can calculate a probability distribution for each row.\n",
    "We can use this as a sanity check.\n",
    "One would hope to see for the most part the class with the\n",
    "highest probability would correspond to the actual class of the row. Another check is that probabilites should sum to 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given\n",
      " x1  x2  x3  x4  x5\n",
      "  0   0   1   1   0\n",
      "the probability distribution is: \n",
      " y       py\n",
      " 1 0.070312\n",
      "-1 0.030864\n",
      "\n",
      "\n",
      "Given\n",
      " x1  x2  x3  x4  x5\n",
      "  1   1   0   1   0\n",
      "the probability distribution is: \n",
      " y      py\n",
      " 1 0.00000\n",
      "-1 0.07716\n",
      "\n",
      "\n",
      "Given\n",
      " x1  x2  x3  x4  x5\n",
      "  0   1   1   1   1\n",
      "the probability distribution is: \n",
      " y      py\n",
      " 1 0.00000\n",
      "-1 0.07716\n",
      "\n",
      "\n",
      "Given\n",
      " x1  x2  x3  x4  x5\n",
      "  1   1   1   1   0\n",
      "the probability distribution is: \n",
      " y       py\n",
      " 1 0.000000\n",
      "-1 0.154321\n",
      "\n",
      "\n",
      "Given\n",
      " x1  x2  x3  x4  x5\n",
      "  0   1   0   0   0\n",
      "the probability distribution is: \n",
      " y       py\n",
      " 1 0.000000\n",
      "-1 0.015432\n",
      "\n",
      "\n",
      "Given\n",
      " x1  x2  x3  x4  x5\n",
      "  1   0   1   1   1\n",
      "the probability distribution is: \n",
      " y       py\n",
      " 1 0.070312\n",
      "-1 0.015432\n",
      "\n",
      "\n",
      "Given\n",
      " x1  x2  x3  x4  x5\n",
      "  0   0   1   0   0\n",
      "the probability distribution is: \n",
      " y       py\n",
      " 1 0.070312\n",
      "-1 0.006173\n",
      "\n",
      "\n",
      "Given\n",
      " x1  x2  x3  x4  x5\n",
      "  1   0   0   0   0\n",
      "the probability distribution is: \n",
      " y       py\n",
      " 1 0.070312\n",
      "-1 0.003086\n",
      "\n",
      "\n",
      "Given\n",
      " x1  x2  x3  x4  x5\n",
      "  1   0   1   1   0\n",
      "the probability distribution is: \n",
      " y       py\n",
      " 1 0.210938\n",
      "-1 0.030864\n",
      "\n",
      "\n",
      "Given\n",
      " x1  x2  x3  x4  x5\n",
      "  1   1   1   1   1\n",
      "the probability distribution is: \n",
      " y      py\n",
      " 1 0.00000\n",
      "-1 0.07716\n",
      "\n",
      "\n",
      "sum of all probabilites is: 0.9798418209876545\n"
     ]
    }
   ],
   "source": [
    "summer = []\n",
    "def getProbOfOutput(row):\n",
    "    def inner(priorRow):\n",
    "        pdfgy = pdf.loc[pdf.y == priorRow.y]\n",
    "        prob = 1\n",
    "        colIdx = 0\n",
    "        for input in row:\n",
    "            prob = prob * pdfgy.iloc[input,colIdx]\n",
    "            colIdx = colIdx + 1\n",
    "        summer.append(prob)\n",
    "        return prob\n",
    "\n",
    "    priorDf['py'] = 0\n",
    "    priorDf['py'] = priorDf.apply(lambda rx: inner(rx) ,axis=1)\n",
    "    print('Given')\n",
    "    print(row.to_frame().T.to_string(index=False))\n",
    "    print(\"the probability distribution is: \")\n",
    "    print(priorDf[['y','py']].to_string(index=False))\n",
    "    print('\\n')\n",
    "\n",
    "ignore = df.drop(columns='y').apply(lambda x: getProbOfOutput(x),axis = 1)\n",
    "print('sum of all probabilites is: ' + str(sum(summer)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<b>Step 4:</b> Prediction\n",
    "Finally we may also use our classifier to predict new data.\n",
    "This process is exactly the same as step 3 only now we may be predicting never before seen data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given\n",
      " x1  x2  x3  x4  x5\n",
      "  1   1   0   1   0\n",
      "the probability distribution is: \n",
      " y      py\n",
      " 1 0.00000\n",
      "-1 0.07716\n",
      "\n",
      "\n",
      "Given\n",
      " x1  x2  x3  x4  x5\n",
      "  0   0   0   0   0\n",
      "the probability distribution is: \n",
      " y       py\n",
      " 1 0.023438\n",
      "-1 0.003086\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inDf = pd.DataFrame({\n",
    "                   'x1':[1,0],\n",
    "                   'x2':[1,0],\n",
    "                   'x3':[0,0],\n",
    "                   'x4':[1,0],\n",
    "                   'x5':[0,0]})\n",
    "ignore = inDf.apply(lambda x: getProbOfOutput(x),axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h4>Part B:</h4>\n",
    "\n",
    "<b>Section 1: Algorithm overview </b>\n",
    "\n",
    "Gaussian Naive Bayes is simply an extension of naive Bayes.\n",
    "Naive Bayes uses the relative frequency of inputs to calculate a simple discrete probability distribution.\n",
    "This idea is extended by instead calculating the mean and standard deviation of inputs with each classification.\n",
    "In doing this we can estimate a continuous probability distribution for each class.\n",
    "Recall the formula of a normal distribution is as follows:\n",
    "\n",
    "$$P(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{ \\frac{-1}{2}\\left( \\frac{x-\\mu}{\\sigma} \\right)^2}$$\n",
    "\n",
    "We can use our training data set to estimate the parameters of the distribution as follows:\n",
    "\n",
    "$$\\mu = \\frac{\\sum x}{n}$$\n",
    "\n",
    "where x is the input variable for some class and n is the frequncy of x in the training data.\n",
    "\n",
    "$$\\sigma = \\sqrt{\\frac{\\sum(x-\\mu)^2}{n}}$$\n",
    "\n",
    "Given these parameters we arrive at an equation that will describe the probability of an event. From this point we follow the same process as we would with a naive bayes classifer.\n",
    "\n",
    "Again in order to find the probability of a class given some input we can apply bayes rule:\n",
    "\n",
    "$$P(y_i|x_1,x_2...,x_n) = P(x_1,x_2,...,x_n) * P(y_i)$$\n",
    "\n",
    "This calculation can be greatly simplified if we make the assumption that all the input variables x are independent.\n",
    "\n",
    "$$P(y_i|x_1,x_2,...,x_n) = P(x_1|y_i)*P(x_2|y_i)*...*P(x_n|y_i)*P(y_i)$$\n",
    "\n",
    "Now we can work with this equation as we can calculate the individual probability distribution terms by plugging in values given some class $y_i$ as follows:\n",
    "\n",
    "let n_i be the number of samples whose class is $y_i$\n",
    "let x_i be any training data with class label y_i such that\n",
    "\n",
    "$$\\mu_i = \\frac{\\sum x}{n_i}$$\n",
    "\n",
    "$$\\sigma_i = \\sqrt{\\frac{\\sum(x_i-\\mu_i)^2}{n_i}}$$\n",
    "\n",
    "\n",
    "$$P(x_i|y_i) = \\frac{1}{\\sigma_i\\sqrt{2\\pi}}e^{ \\frac{-1}{2}\\left( \\frac{x-\\mu_i}{\\sigma_i} \\right)^2}$$\n",
    "\n",
    "Lastly $P(y_i)$ can simply be calculated as the relative frequcy of the class:\n",
    "\n",
    "$$P(y_i) = \\frac{count(y_i)}{count(y)}$$\n",
    "\n",
    "With all the pieces of the puzzle together we would be able to apply the following formula to find the class probabilities\n",
    "\n",
    "$$P(y_i|x_1,x_2...,x_n) = P(x_1,x_2,...,x_n) * P(y_i)$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<b>Section 2: Implantation overview </b>\n",
    "\n",
    "In the following section a Niave Gaussian Bayes code implementation will be used to classify breast cancer samples into malignant or begnin.\n",
    "There are 30 features which are used to inform the classification.\n",
    "More information on the dataset may be found at the following:\n",
    "<href>https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html</href>\n",
    "\n",
    "<b>Section 2 a: Implantation: </b> Gaussian Naive Bayes Classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean accuracy of the classifier is: 0.9473684210526315\n",
      "The confusion matrix is: \n",
      "[[30  4]\n",
      " [ 2 78]]\n"
     ]
    }
   ],
   "source": [
    "data = load_breast_cancer(as_frame=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data,data.target,test_size=.2)\n",
    "ngclf = GaussianNB()\n",
    "ngclf.fit(X_train,y_train)\n",
    "print('mean accuracy of the classifier is: ' + str(ngclf.score(X_test,y_test)))\n",
    "print('The confusion matrix is: ')\n",
    "print(str(confusion_matrix(y_test, ngclf.predict(X_test))))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<b>Section 2 b: Implantation: The effects of noise</b>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introducing normally distributed noise to all features: \n",
      "mean accuracy of the classifier @ sigma = 2.23606797749979 is: 0.9385964912280702\n",
      "The confusion matrix @ sigma = 2.23606797749979 is: \n",
      "[[37  4]\n",
      " [ 3 70]]\n",
      "\n",
      "\n",
      "mean accuracy of the classifier @ sigma = 22.360679774997898 is: 0.9298245614035088\n",
      "The confusion matrix @ sigma = 22.360679774997898 is: \n",
      "[[35  6]\n",
      " [ 2 71]]\n",
      "\n",
      "\n",
      "mean accuracy of the classifier @ sigma = 70.71067811865476 is: 0.9035087719298246\n",
      "The confusion matrix @ sigma = 70.71067811865476 is: \n",
      "[[32  9]\n",
      " [ 2 71]]\n",
      "\n",
      "\n",
      "mean accuracy of the classifier @ sigma = 223.60679774997897 is: 0.8947368421052632\n",
      "The confusion matrix @ sigma = 223.60679774997897 is: \n",
      "[[31 10]\n",
      " [ 2 71]]\n",
      "\n",
      "\n",
      "mean accuracy of the classifier @ sigma = 707.1067811865476 is: 0.8157894736842105\n",
      "The confusion matrix @ sigma = 707.1067811865476 is: \n",
      "[[20 21]\n",
      " [ 0 73]]\n",
      "\n",
      "\n",
      "mean accuracy of the classifier @ sigma = 2236.06797749979 is: 0.6578947368421053\n",
      "The confusion matrix @ sigma = 2236.06797749979 is: \n",
      "[[ 2 39]\n",
      " [ 0 73]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "noiseVarince = [5,5e2,5e3,5e4,5e5,5e6]\n",
    "noiseSigma = np.sqrt(noiseVarince)\n",
    "acc = []\n",
    "print('Introducing normally distributed noise to all features: ')\n",
    "for s in noiseSigma:\n",
    "    noisy_X_train = X_train.apply(lambda x: x + np.random.normal(0,s,len(X_train)))\n",
    "    ngclf.fit(noisy_X_train,y_train)\n",
    "    acc.append(ngclf.score(X_test,y_test))\n",
    "    print('mean accuracy of the classifier @ sigma = '+str(s) + ' is: ' + str(ngclf.score(X_test,y_test)))\n",
    "    print('The confusion matrix @ sigma = '+str(s) + ' is: ')\n",
    "    print(str(confusion_matrix(y_test, ngclf.predict(X_test))))\n",
    "    print('\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<b>Section 3 a: Experimental results </b>\n",
    "\n",
    "For this data set the Gaussian naive bayes classifier works quite well.\n",
    "On an 80 20 test train split the accuracy of the classifier is between 90 and 95 percent.\n",
    "This would suggest at least for relatively simple binary classification problems the gaussian Naive Bayes classifier is effective.\n",
    "There are some underlying assumptions that may cause this classifier to fail.\n",
    "The biggest one is that all features are independent.\n",
    "For the case of the Wisconsin cancer data set this is probably a fair assumption as humans selected features they thought were relevant.\n",
    "In many applications this would not be the case, and in general this is seen as one of the greatest short comings of this method.\n",
    "Also in our case the feature dimension is very large compared to the class dimension.\n",
    "This means that even if our independence assumption is only true for a small subset of the features we can probably still get good performance.\n",
    "\n",
    "\n",
    "In section 2b we se the classifier is somewhat sensitive to noise.\n",
    "One nice thing about the bayes approach, it is not necessary to scale all the features into the same range.\n",
    "This means that if we add a noise signal across all the features some may be affected more than others.\n",
    "As the size of the noise grows more of the features will start to represent the noisy gausian distributions rather than their own true underlying one.\n",
    "This makes this method somewhat sensitive to noise.\n",
    "As we can see for see from the graph of accuracy vs standard deviation of a noise distribution below,\n",
    "the accuracy is not hurt much for small amounts of noise but as sigma grows the performance is hurt greatly.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogx(noiseSigma,acc)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "There is a fairly steep drop off in accuracy around sigma = 500.\n",
    "Past that point the classifier begins to approach a random guess.\n",
    "For a binary classification simply guessing the class would result in an accuracy of around 50%.\n",
    "For the largest sigma value of noise the accuracy of classifier was around 65%.\n",
    "\n",
    "<b>Section 3 b: Discussion </b>\n",
    "\n",
    "The naive Bayes classifier is simple and extremely fast compared to many other classification methods.\n",
    "The classifier does not need to take many gradient decent or other optimization steps to learn.\n",
    "It only requires fairly simple operations to be preformed once.\n",
    "Another large advantage is that it is not necessary to retrain the whole model when new data is available it is trivial to recalculate the new average and standard devation.\n",
    "For simple or exploratory data analysis naive bayes is certainly a powerful tool.\n",
    "That being said the classifier makes many assumptions that simply do not reflect the real world.\n",
    "The biggest one is that all features are independent.\n",
    "For some problems this alone could be reason enough to avoid the classifier compleatly.\n",
    "Another challenge with the bayes classifier is that if there is not enough data then some conditonal probabilites may be 0.\n",
    "Because we assumed independence this would make our whole classifier base its decision on 1 of potentially many features.\n",
    "In order to combat this we can use a laplace estimator.\n",
    "Essentially this works by adding a small constant to the relative frequencies of the inputs.\n",
    "This helps overcome this problem but still can lead to some features being heavly weighted.\n",
    "A common scheme is as follows.\n",
    "\n",
    "$$P(x_i|y_i)= \\frac{count(x_i)+m*p}{count(y_i)+m}$$\n",
    "\n",
    "where m is the number of unique values of a given feature and\n",
    "p is the prior estimate of the feature distribution.\n",
    "For the uniform case this is $p=1/m$. Another common form of the equation is then:\n",
    "\n",
    "$$P(x_i|y_i)= \\frac{count(x_i)+1}{count(y_i)+m}$$"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}